"""
è®­ç»ƒ Box Refinement Module - ä¼˜åŒ–ç‰ˆæœ¬
ç›‘ç£ä¿¡å·: Ground Truth Mask çš„æœ€å°å¤–æ¥çŸ©å½¢ (ä½œä¸º target bbox)

ä¼˜åŒ–åŠŸèƒ½:
1. HQ-SAM ç‰¹å¾ç¼“å­˜æœºåˆ¶ - é¿å…é‡å¤è®¡ç®—
2. æ•°æ®æŠ½æ ·å‚æ•° - å‡å°‘è®­ç»ƒæ•°æ®é‡
3. æ··åˆç²¾åº¦è®­ç»ƒ - åŠ é€Ÿè®­ç»ƒ
4. è‡ªåŠ¨æ£€æµ‹ç‰¹å¾æ–‡ä»¶å¤¹ - æ™ºèƒ½ç¼“å­˜ç®¡ç†
5. --fast æ¨¡å¼ - ä¸€é”®å¯ç”¨æ‰€æœ‰ä¼˜åŒ–
"""

import os
import sys
import yaml
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
import hashlib
import random
import time

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.cuda.amp as amp
import cv2
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

# æ·»åŠ modulesç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'modules'))

from modules.box_refinement import BoxRefinementModule, box_iou_loss, visualize_refinement
from modules.hqsam_feature_extractor import create_hqsam_extractor


class FeatureCache:
    """HQ-SAM ç‰¹å¾ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str, split: str = 'train'):
        """
        Args:
            cache_dir: ç¼“å­˜ç›®å½•æ ¹è·¯å¾„
            split: æ•°æ®é›†åˆ†å‰² ('train' æˆ– 'val')
        """
        self.cache_dir = Path(cache_dir) / f"features/{split}"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.split = split
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.cache_hits = 0
        self.cache_misses = 0
        self.total_requests = 0
    
    def get_cache_path(self, image_path: str) -> Path:
        """è·å–å›¾åƒå¯¹åº”çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨å›¾åƒè·¯å¾„çš„å“ˆå¸Œå€¼ä½œä¸ºæ–‡ä»¶å
        image_hash = hashlib.md5(image_path.encode()).hexdigest()
        return self.cache_dir / f"{image_hash}.npy"
    
    def load_features(self, image_path: str) -> Optional[torch.Tensor]:
        """ä»ç¼“å­˜åŠ è½½ç‰¹å¾"""
        self.total_requests += 1
        cache_path = self.get_cache_path(image_path)
        
        if cache_path.exists():
            try:
                features = np.load(cache_path)
                features_tensor = torch.from_numpy(features)
                self.cache_hits += 1
                return features_tensor
            except Exception as e:
                print(f"Warning: Failed to load cached features from {cache_path}: {e}")
                self.cache_misses += 1
                return None
        else:
            self.cache_misses += 1
            return None
    
    def save_features(self, image_path: str, features: torch.Tensor):
        """ä¿å­˜ç‰¹å¾åˆ°ç¼“å­˜"""
        cache_path = self.get_cache_path(image_path)
        try:
            # ç¡®ä¿ç‰¹å¾åœ¨CPUä¸Š
            features_cpu = features.cpu().numpy()
            np.save(cache_path, features_cpu)
        except Exception as e:
            print(f"Warning: Failed to save features to {cache_path}: {e}")
    
    def get_cache_stats(self) -> Dict[str, float]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.total_requests == 0:
            return {"hit_rate": 0.0, "hits": 0, "misses": 0, "total": 0}
        
        hit_rate = self.cache_hits / self.total_requests
        return {
            "hit_rate": hit_rate,
            "hits": self.cache_hits,
            "misses": self.cache_misses,
            "total": self.total_requests
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.cache_dir.exists():
            import shutil
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_hits = 0
        self.cache_misses = 0
        self.total_requests = 0


class FungiDataset(Dataset):
    """FungiTastic æ•°æ®é›†åŠ è½½å™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, data_root: str, split: str = 'train', 
                 image_size: int = 300, data_subset: str = 'Mini',
                 augmentation: bool = True, debug: bool = False,
                 masks_file: Optional[str] = None,
                 sample_ratio: Optional[float] = None,
                 feature_cache: Optional[FeatureCache] = None):
        """
        Args:
            data_root: æ•°æ®é›†æ ¹ç›®å½•
            split: 'train' æˆ– 'val'
            image_size: å›¾åƒå°ºå¯¸
            data_subset: 'Mini' æˆ– 'Full'
            augmentation: æ˜¯å¦å¯ç”¨æ•°æ®å¢å¼º
            debug: è°ƒè¯•æ¨¡å¼
            masks_file: æ©ç æ–‡ä»¶è·¯å¾„
            sample_ratio: æ•°æ®æŠ½æ ·æ¯”ä¾‹ (0.0-1.0)
            feature_cache: ç‰¹å¾ç¼“å­˜ç®¡ç†å™¨
        """
        self.data_root = Path(data_root)
        self.split = split
        self.image_size = image_size
        self.data_subset = data_subset
        self.augmentation = augmentation
        self.debug = debug
        self.sample_ratio = sample_ratio
        self.feature_cache = feature_cache
        
        self.images_dir = self.data_root / f"{data_subset}" / split / f"{image_size}p"

        # å¦‚æä¾›parquetè·¯å¾„åˆ™ä½¿ç”¨
        if masks_file:
            self.masks_path = Path(masks_file)
            if not self.masks_path.exists():
                raise FileNotFoundError(f"Masks parquet file not found: {self.masks_path}")
            self.use_parquet_masks = True
            # ä½¿ç”¨ pyarrow.dataset è¿›è¡ŒæŒ‰éœ€è¯»å–ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å…¨è¡¨
            try:
                import pyarrow.dataset as ds  # type: ignore
                self._pa_ds = ds.dataset(str(self.masks_path), format='parquet')
                # è®°å½•å¯ç”¨åˆ—
                self._pa_schema_names = set(self._pa_ds.schema.names)
                self._pa_has_mask = 'mask' in self._pa_schema_names
                self._pa_has_rle = all(name in self._pa_schema_names for name in ['rle', 'width', 'height'])
                self._pa_file_name_field = 'file_name' if 'file_name' in self._pa_schema_names else None
            except Exception as e:
                raise RuntimeError(f"Failed to open parquet dataset: {e}")
            # ç®€å•çš„æœ€è¿‘ä½¿ç”¨ç¼“å­˜ï¼Œå‡å°‘é‡å¤IO
            self._mask_cache = {}
            self._mask_cache_limit = 1024
        else:
            self.masks_dir = self.data_root / f"{data_subset}" / split / "masks"
            if not self.masks_dir.exists():
                raise FileNotFoundError(f"Masks directory not found: {self.masks_dir}")
            self.use_parquet_masks = False

        
        # è·å–å›¾åƒæ–‡ä»¶åˆ—è¡¨
        self.image_files = sorted(list(self.images_dir.glob("*.jpg")) + 
                                 list(self.images_dir.glob("*.JPG")) + 
                                 list(self.images_dir.glob("*.png")))
        
        if debug:
            self.image_files = self.image_files[:100]  # è°ƒè¯•æ¨¡å¼åªä½¿ç”¨å‰100å¼ å›¾åƒ
        elif sample_ratio is not None and sample_ratio < 1.0:
            # æ•°æ®æŠ½æ ·
            num_samples = int(len(self.image_files) * sample_ratio)
            self.image_files = random.sample(self.image_files, num_samples)
            print(f"Sampled {len(self.image_files)} images from {len(self.image_files) // sample_ratio:.0f} total images (ratio: {sample_ratio})")
        
        print(f"Found {len(self.image_files)} images in {self.split} split")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image_path = self.image_files[idx]
        image = cv2.imread(str(image_path))
        if image is None:
            raise ValueError(f"Failed to load image: {image_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        gt_bbox = None
        if self.use_parquet_masks:
            # ä¼˜å…ˆä»ç¼“å­˜è·å–ï¼ˆä½¿ç”¨æ–‡ä»¶åä½œä¸ºé”®ï¼‰
            image_key = image_path.name
            cached = self._mask_cache.get(image_key)
            if cached is not None:
                gt_bbox = cached
            else:
                try:
                    import pyarrow.dataset as ds  # type: ignore
                except Exception as e:
                    raise RuntimeError(f"pyarrow is required for parquet reading: {e}")
                # ä»…è¯·æ±‚å®é™…å­˜åœ¨çš„åˆ—
                requested_cols = []
                if self._pa_file_name_field is not None:
                    requested_cols.append(self._pa_file_name_field)
                if self._pa_has_mask:
                    requested_cols.append('mask')
                if self._pa_has_rle:
                    requested_cols.extend(['width', 'height', 'rle'])
                if self._pa_file_name_field is not None:
                    filter_expr = (ds.field(self._pa_file_name_field) == image_key)
                else:
                    filter_expr = None
                table = self._pa_ds.to_table(columns=requested_cols, filter=filter_expr) if filter_expr is not None else self._pa_ds.to_table(columns=requested_cols)
                if table.num_rows == 0 and self._pa_file_name_field is not None:
                    filter_expr2 = (ds.field(self._pa_file_name_field) == image_path.stem)
                    table = self._pa_ds.to_table(columns=requested_cols, filter=filter_expr2)
                if table.num_rows == 0:
                    gt_bbox = None
                else:
                    cols = {name: table.column(name) for name in table.schema.names}
                    if self._pa_has_rle and all(k in cols for k in ['rle', 'width', 'height']):
                        rle_list = cols['rle'][0].as_py()
                        width_val = int(cols['width'][0].as_py())
                        height_val = int(cols['height'][0].as_py())
                        gt_bbox = self._compute_bbox_from_rle_counts(rle_list, width_val, height_val)
                    elif self._pa_has_mask and 'mask' in cols:
                        cell = cols['mask'][0].as_py()
                        mask_arr = np.asarray(cell)
                        if mask_arr.ndim == 3:
                            mask_arr = mask_arr[..., 0]
                        if mask_arr.ndim == 1:
                            # 1D æƒ…å†µï¼ŒæŒ‰å½“å‰å›¾åƒå°ºå¯¸é‡å¡‘
                            mask_arr = mask_arr.reshape(image.shape[0], image.shape[1])
                        gt_bbox = self._compute_bbox_from_mask(mask_arr.astype(np.uint8))
                # ç¼“å­˜bbox
                if gt_bbox is None:
                    gt_bbox = self._compute_bbox_from_mask(np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8))
                if len(self._mask_cache) >= self._mask_cache_limit:
                    self._mask_cache.pop(next(iter(self._mask_cache)))
                self._mask_cache[image_key] = gt_bbox
        else:
            mask_path = self.masks_dir / f"{image_path.stem}.png"
            if not mask_path.exists():
                mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
                gt_bbox = self._compute_bbox_from_mask(mask)
            else:
                mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
            if mask is None:
                mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
            if gt_bbox is None:
                gt_bbox = self._compute_bbox_from_mask(mask)
        
        # è°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œå¹¶æŒ‰æ¯”ä¾‹ç¼©æ”¾ bboxï¼ˆé¿å…é‡å»ºæ•´å¼ maskï¼‰
        orig_h, orig_w = image.shape[:2]
        if image.shape[:2] != (self.image_size, self.image_size):
            sx = self.image_size / orig_w
            sy = self.image_size / orig_h
            image = cv2.resize(image, (self.image_size, self.image_size))
            if gt_bbox is not None:
                x1, y1, x2, y2 = gt_bbox
                gt_bbox = np.array([x1 * sx, y1 * sy, x2 * sx, y2 * sy], dtype=np.float32)
            # ä¸å†éœ€è¦maskå‚ä¸è®­ç»ƒï¼Œæä¾›å ä½å³å¯
            mask = np.zeros((self.image_size, self.image_size), dtype=np.uint8)
        else:
            # ä¸ä½¿ç”¨çœŸå®maskä»¥èŠ‚çœCPUæ—¶é—´
            mask = np.zeros((orig_h, orig_w), dtype=np.uint8)
        
        # è‹¥ä»æœªå¾—åˆ°bboxï¼Œå…œåº•ä½¿ç”¨ç©ºmaskè§„åˆ™
        if gt_bbox is None:
            gt_bbox = self._compute_bbox_from_mask(mask)
        
        # ç”Ÿæˆnoisy bbox (æ¨¡æ‹ŸYOLOè¾“å‡º)
        noisy_bbox = self._generate_noisy_bbox(gt_bbox, image.shape[:2])
        
        return {
            'image': image,
            'mask': mask,
            'gt_bbox': gt_bbox,
            'noisy_bbox': noisy_bbox,
            'image_path': str(image_path)
        }

    def _compute_bbox_from_rle_counts(self, counts: List[int], width: int, height: int) -> np.ndarray:
        """ä»RLEè®¡æ•°ç›´æ¥è®¡ç®—bboxï¼Œé¿å…å±•å¼€æ•´å›¾ã€‚
        å‡è®¾countsäº¤æ›¿ä»£è¡¨0/1åƒç´ æ®µé•¿åº¦ï¼Œèµ·å§‹ä¸º0ï¼ˆèƒŒæ™¯ï¼‰ã€‚
        """
        total = width * height
        pos = 0
        value = 0
        min_row, min_col = height, width
        max_row, max_col = -1, -1
        for c in counts:
            if c <= 0:
                continue
            if value == 1:
                start = pos
                end = min(pos + int(c) - 1, total - 1)
                # è®¡ç®—è¯¥å‰æ™¯æ®µçš„è¡Œåˆ—èŒƒå›´
                start_row, start_col = divmod(start, width)
                end_row, end_col = divmod(end, width)
                # æ›´æ–°bbox
                if start_row < min_row:
                    min_row = start_row
                if end_row > max_row:
                    max_row = end_row
                if start_col < min_col:
                    min_col = start_col
                if end_col > max_col:
                    max_col = end_col
            pos += int(c)
            value = 1 - value
            if pos >= total:
                break
        if max_row < 0:
            # æ²¡æœ‰å‰æ™¯
            center_x, center_y = width // 2, height // 2
            size = min(width, height) // 10
            return np.array([center_x - size, center_y - size, center_x + size, center_y + size], dtype=np.float32)
        # å°†è¡Œåˆ—è½¬ä¸ºx1,y1,x2,y2
        x1 = float(min_col)
        y1 = float(min_row)
        x2 = float(max_col)
        y2 = float(max_row)
        return np.array([x1, y1, x2, y2], dtype=np.float32)
    
    def _compute_bbox_from_mask(self, mask: np.ndarray) -> np.ndarray:
        """ä»maskè®¡ç®—æœ€å°å¤–æ¥çŸ©å½¢"""
        # æ‰¾åˆ°éé›¶åƒç´ 
        coords = np.column_stack(np.where(mask > 0))
        
        if len(coords) == 0:
            # å¦‚æœæ²¡æœ‰å‰æ™¯åƒç´ ï¼Œè¿”å›ä¸­å¿ƒçš„å°çŸ©å½¢
            h, w = mask.shape
            center_x, center_y = w // 2, h // 2
            size = min(w, h) // 10
            return np.array([center_x - size, center_y - size, 
                           center_x + size, center_y + size], dtype=np.float32)
        
        # è®¡ç®—è¾¹ç•Œæ¡†
        x1, y1 = coords.min(axis=0)[::-1]  # æ³¨æ„åæ ‡é¡ºåº
        x2, y2 = coords.max(axis=0)[::-1]
        
        return np.array([x1, y1, x2, y2], dtype=np.float32)
    
    def _generate_noisy_bbox(self, gt_bbox: np.ndarray, img_shape: Tuple[int, int]) -> np.ndarray:
        """ç”Ÿæˆå¸¦å™ªå£°çš„bbox (æ¨¡æ‹ŸYOLOè¾“å‡º)"""
        h, w = img_shape
        
        # æ·»åŠ éšæœºåç§»å’Œç¼©æ”¾
        noise_scale = 0.1  # 10%çš„å™ªå£°
        offset_scale = 20   # æœ€å¤§20åƒç´ çš„åç§»
        
        x1, y1, x2, y2 = gt_bbox
        
        # è®¡ç®—ä¸­å¿ƒç‚¹å’Œå°ºå¯¸
        cx = (x1 + x2) / 2
        cy = (y1 + y2) / 2
        bw = x2 - x1
        bh = y2 - y1
        
        # æ·»åŠ å™ªå£°
        cx_noise = np.random.normal(0, offset_scale)
        cy_noise = np.random.normal(0, offset_scale)
        scale_noise = np.random.normal(1.0, noise_scale)
        
        # åº”ç”¨å™ªå£°
        new_cx = cx + cx_noise
        new_cy = cy + cy_noise
        new_bw = bw * scale_noise
        new_bh = bh * scale_noise
        
        # è®¡ç®—æ–°çš„bbox
        new_x1 = max(0, new_cx - new_bw / 2)
        new_y1 = max(0, new_cy - new_bh / 2)
        new_x2 = min(w - 1, new_cx + new_bw / 2)
        new_y2 = min(h - 1, new_cy + new_bh / 2)
        
        return np.array([new_x1, new_y1, new_x2, new_y2], dtype=np.float32)


def collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°"""
    images = torch.stack([torch.from_numpy(item['image']).permute(2, 0, 1).float() / 255.0 
                         for item in batch])
    masks = torch.stack([torch.from_numpy(item['mask']).float() for item in batch])
    gt_bboxes = torch.stack([torch.from_numpy(item['gt_bbox']).float() for item in batch])
    noisy_bboxes = torch.stack([torch.from_numpy(item['noisy_bbox']).float() for item in batch])
    image_paths = [item['image_path'] for item in batch]
    
    return {
        'images': images,
        'masks': masks,
        'gt_bboxes': gt_bboxes,
        'noisy_bboxes': noisy_bboxes,
        'image_paths': image_paths
    }


def compute_loss(pred_bboxes, gt_bboxes, l1_weight=1.0, iou_weight=2.0):
    """è®¡ç®—æŸå¤±å‡½æ•°"""
    # L1æŸå¤±
    l1_loss = nn.L1Loss()(pred_bboxes, gt_bboxes)
    
    # IoUæŸå¤±
    iou_loss = box_iou_loss(pred_bboxes, gt_bboxes)
    
    # æ€»æŸå¤±
    total_loss = l1_weight * l1_loss + iou_weight * iou_loss
    
    return total_loss, l1_loss, iou_loss


def extract_features_with_cache(hqsam_extractor, images_np_list, image_paths, feature_cache):
    """ä½¿ç”¨ç¼“å­˜æå–ç‰¹å¾"""
    features_list = []
    
    for i, (image_np, image_path) in enumerate(zip(images_np_list, image_paths)):
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if feature_cache is not None:
            cached_features = feature_cache.load_features(image_path)
            if cached_features is not None:
                features_list.append(cached_features)
                continue
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæå–ç‰¹å¾
        features = hqsam_extractor.extract_features(image_np)
        features_list.append(features)
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if feature_cache is not None:
            feature_cache.save_features(image_path, features)
    
    return features_list


def train_one_epoch(model, dataloader, optimizer, hqsam_extractor, device, epoch, config, 
                   feature_cache=None, use_amp=False):
    """è®­ç»ƒä¸€ä¸ªepoch - ä¼˜åŒ–ç‰ˆæœ¬"""
    model.train()
    total_loss = 0
    total_l1_loss = 0
    total_iou_loss = 0
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = amp.GradScaler() if use_amp else None
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
    
    for batch_idx, batch in enumerate(pbar):
        images = batch['images'].to(device)
        gt_bboxes = batch['gt_bboxes'].to(device)
        noisy_bboxes = batch['noisy_bboxes'].to(device)
        image_paths = batch['image_paths']
        
        # æå–å›¾åƒç‰¹å¾ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        images_np_list = []
        for i in range(images.shape[0]):
            img_np = images[i].permute(1, 2, 0).cpu().numpy()
            img_np = (img_np * 255).astype(np.uint8)
            images_np_list.append(img_np)
        
        features_list = extract_features_with_cache(hqsam_extractor, images_np_list, image_paths, feature_cache)
        image_features = torch.cat(features_list, dim=0)  # (B, 256, 64, 64)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        
        if use_amp and scaler is not None:
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with amp.autocast():
                # è¿­ä»£ç²¾ç‚¼
                refined_bboxes, history = model.iterative_refine(
                    image_features, noisy_bboxes, (config['data']['image_size'], config['data']['image_size']),
                    max_iter=config['refinement']['max_iter'],
                    stop_threshold=config['refinement']['stop_threshold']
                )
                
                # è®¡ç®—æŸå¤±
                loss, l1_loss, iou_loss = compute_loss(
                    refined_bboxes, gt_bboxes,
                    l1_weight=config['loss']['l1_weight'],
                    iou_weight=config['loss']['iou_weight']
                )
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            # æ™®é€šç²¾åº¦å‰å‘ä¼ æ’­
            # è¿­ä»£ç²¾ç‚¼
            refined_bboxes, history = model.iterative_refine(
                image_features, noisy_bboxes, (config['data']['image_size'], config['data']['image_size']),
                max_iter=config['refinement']['max_iter'],
                stop_threshold=config['refinement']['stop_threshold']
            )
            
            # è®¡ç®—æŸå¤±
            loss, l1_loss, iou_loss = compute_loss(
                refined_bboxes, gt_bboxes,
                l1_weight=config['loss']['l1_weight'],
                iou_weight=config['loss']['iou_weight']
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
        
        # æ›´æ–°ç»Ÿè®¡
        total_loss += loss.item()
        total_l1_loss += l1_loss.item()
        total_iou_loss += iou_loss.item()
        
        # æ›´æ–°è¿›åº¦æ¡
        cache_stats = feature_cache.get_cache_stats() if feature_cache else {}
        cache_info = f"Cache: {cache_stats.get('hit_rate', 0):.1%}" if cache_stats else ""
        
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'L1': f'{l1_loss.item():.4f}',
            'IoU': f'{iou_loss.item():.4f}',
            'Cache': cache_info
        })
        
        # å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
        if config['output']['vis_freq'] and config['output']['vis_freq'] > 0 and (batch_idx % config['output']['vis_freq'] == 0):
            visualize_batch(model, batch, image_features, hqsam_extractor, 
                          config, epoch, batch_idx)
    
    return total_loss / len(dataloader), total_l1_loss / len(dataloader), total_iou_loss / len(dataloader)


def visualize_batch(model, batch, image_features, hqsam_extractor, config, epoch, batch_idx):
    """å¯è§†åŒ–ä¸€ä¸ªbatchçš„ç»“æœ"""
    device = next(model.parameters()).device
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    image = batch['images'][0].permute(1, 2, 0).cpu().numpy()
    image = (image * 255).astype(np.uint8)
    # è®­ç»ƒè®¡ç®—ä¿æŒä¸ºtensorï¼Œä»…åœ¨ç»˜å›¾æ—¶è½¬æ¢ä¸ºnumpy
    gt_bbox_t = batch['gt_bboxes'][0].to(device).float()
    noisy_bbox_t = batch['noisy_bboxes'][0].to(device).float()
    
    # è·å–ç²¾ç‚¼åçš„bbox
    with torch.no_grad():
        refined_bbox, history = model.iterative_refine(
            image_features[0:1], noisy_bbox_t[None, :],
            (config['data']['image_size'], config['data']['image_size']),
            max_iter=config['refinement']['max_iter']
        )
        refined_bbox_np = refined_bbox[0].cpu().numpy()
    gt_bbox = gt_bbox_t.cpu().numpy()
    noisy_bbox = noisy_bbox_t.cpu().numpy()
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # åŸå§‹å›¾åƒ + GT bbox
    axes[0].imshow(image)
    gt_rect = plt.Rectangle((gt_bbox[0], gt_bbox[1]), gt_bbox[2]-gt_bbox[0], gt_bbox[3]-gt_bbox[1],
                           linewidth=2, edgecolor='green', facecolor='none', label='GT')
    axes[0].add_patch(gt_rect)
    axes[0].set_title('Ground Truth')
    axes[0].axis('off')
    
    # åŸå§‹å›¾åƒ + Noisy bbox
    axes[1].imshow(image)
    noisy_rect = plt.Rectangle((noisy_bbox[0], noisy_bbox[1]), noisy_bbox[2]-noisy_bbox[0], noisy_bbox[3]-noisy_bbox[1],
                              linewidth=2, edgecolor='red', facecolor='none', label='Noisy')
    axes[1].add_patch(noisy_rect)
    axes[1].set_title('Noisy (YOLO)')
    axes[1].axis('off')
    
    # åŸå§‹å›¾åƒ + Refined bbox
    axes[2].imshow(image)
    refined_rect = plt.Rectangle((refined_bbox_np[0], refined_bbox_np[1]), refined_bbox_np[2]-refined_bbox_np[0], refined_bbox_np[3]-refined_bbox_np[1],
                                linewidth=2, edgecolor='blue', facecolor='none', label='Refined')
    axes[2].add_patch(refined_rect)
    axes[2].set_title('Refined')
    axes[2].axis('off')
    
    plt.suptitle(f'Epoch {epoch}, Batch {batch_idx}')
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    vis_dir = Path(config['output']['visualization_dir'])
    vis_dir.mkdir(parents=True, exist_ok=True)
    plt.savefig(vis_dir / f'epoch_{epoch}_batch_{batch_idx}.png', dpi=150, bbox_inches='tight')
    plt.close()


def evaluate(model, dataloader, hqsam_extractor, device, config, feature_cache=None):
    """è¯„ä¼°æ¨¡å‹ - ä¼˜åŒ–ç‰ˆæœ¬"""
    model.eval()
    total_loss = 0
    total_l1_loss = 0
    total_iou_loss = 0
    ious = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            images = batch['images'].to(device)
            gt_bboxes = batch['gt_bboxes'].to(device)
            noisy_bboxes = batch['noisy_bboxes'].to(device)
            image_paths = batch['image_paths']
            
            # æå–å›¾åƒç‰¹å¾ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            images_np_list = []
            for i in range(images.shape[0]):
                image_np = images[i].permute(1, 2, 0).cpu().numpy()
                image_np = (image_np * 255).astype(np.uint8)
                images_np_list.append(image_np)
            
            features_list = extract_features_with_cache(hqsam_extractor, images_np_list, image_paths, feature_cache)
            image_features = torch.cat(features_list, dim=0)
            
            # å‰å‘ä¼ æ’­
            refined_bboxes, _ = model.iterative_refine(
                image_features, noisy_bboxes, 
                (config['data']['image_size'], config['data']['image_size']),
                max_iter=config['refinement']['max_iter']
            )
            
            # è®¡ç®—æŸå¤±
            loss, l1_loss, iou_loss = compute_loss(
                refined_bboxes, gt_bboxes,
                l1_weight=config['loss']['l1_weight'],
                iou_weight=config['loss']['iou_weight']
            )
            
            total_loss += loss.item()
            total_l1_loss += l1_loss.item()
            total_iou_loss += iou_loss.item()
            
            # è®¡ç®—IoU
            for i in range(refined_bboxes.shape[0]):
                iou = compute_bbox_iou(refined_bboxes[i], gt_bboxes[i])
                ious.append(iou.item())
    
    avg_loss = total_loss / len(dataloader)
    avg_l1_loss = total_l1_loss / len(dataloader)
    avg_iou_loss = total_iou_loss / len(dataloader)
    avg_iou = np.mean(ious)
    
    return avg_loss, avg_l1_loss, avg_iou_loss, avg_iou


def compute_bbox_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªbboxçš„IoU"""
    x1 = torch.max(box1[0], box2[0])
    y1 = torch.max(box1[1], box2[1])
    x2 = torch.min(box1[2], box2[2])
    y2 = torch.min(box1[3], box2[3])
    
    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    iou = intersection / (union + 1e-6)
    return iou


def detect_feature_cache(data_root: str, split: str) -> bool:
    """æ£€æµ‹æ˜¯å¦å­˜åœ¨ç‰¹å¾ç¼“å­˜æ–‡ä»¶å¤¹"""
    cache_dir = Path(data_root) / f"features/{split}"
    return cache_dir.exists() and len(list(cache_dir.glob("*.npy"))) > 0


def main():
    parser = argparse.ArgumentParser(description="Train Box Refinement Module - Optimized")
    parser.add_argument("--config", required=True, help="Path to config file")
    parser.add_argument("--resume", help="Path to checkpoint to resume from")
    parser.add_argument("--debug", action="store_true", help="Debug mode")
    parser.add_argument("--fast", action="store_true", help="Enable fast mode (all optimizations)")
    parser.add_argument("--clear-cache", action="store_true", help="Clear feature cache before training")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # åº”ç”¨ fast æ¨¡å¼è®¾ç½®
    if args.fast:
        print("ğŸš€ Fast mode enabled - applying all optimizations")
        config['data']['sample_ratio'] = config['data'].get('sample_ratio', 0.1)
        config['training']['use_amp'] = True
        config['training']['batch_size'] = min(config['training']['batch_size'] * 2, 32)  # å¢åŠ batch sizeä½†ä¸è¶…è¿‡32
        print(f"  - Data sampling: {config['data']['sample_ratio']}")
        print(f"  - Mixed precision: {config['training'].get('use_amp', True)}")
        print(f"  - Batch size: {config['training']['batch_size']}")
    
    if args.debug:
        config['debug']['enabled'] = True
        config['training']['epochs'] = 5
        config['training']['batch_size'] = 4
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cuda')
    print(f"Using device: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    for dir_name in ['save_dir', 'log_dir', 'checkpoint_dir', 'visualization_dir']:
        Path(config['output'][dir_name]).mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(Path(config['output']['log_dir']) / 'training.log'),
            logging.StreamHandler()
        ]
    )
    
    # æ£€æµ‹ç‰¹å¾ç¼“å­˜
    cache_available = detect_feature_cache(config['data']['data_root'], config['data']['train_split'])
    print(f"Feature cache detected: {cache_available}")
    
    # åˆ›å»ºç‰¹å¾ç¼“å­˜ç®¡ç†å™¨
    feature_cache = FeatureCache(config['output']['save_dir'], config['data']['train_split'])
    if args.clear_cache:
        print("Clearing feature cache...")
        feature_cache.clear_cache()
    
    # åˆ›å»ºæ•°æ®é›†
    print("Loading datasets...")
    train_dataset = FungiDataset(
        data_root=config['data']['data_root'],
        split=config['data']['train_split'],
        image_size=config['data']['image_size'],
        data_subset=config['data']['data_subset'],
        augmentation=config['data']['augmentation']['enabled'],
        debug=config['debug']['enabled'],
        masks_file=config['data'].get('masks_file'),
        sample_ratio=config['data'].get('sample_ratio'),
        feature_cache=feature_cache
    )
    
    val_dataset = FungiDataset(
        data_root=config['data']['data_root'],
        split=config['data']['val_split'],
        image_size=config['data']['image_size'],
        data_subset=config['data']['data_subset'],
        augmentation=False,
        debug=config['debug']['enabled'],
        masks_file=config['data'].get('masks_file'),
        sample_ratio=config['data'].get('sample_ratio'),
        feature_cache=FeatureCache(config['output']['save_dir'], config['data']['val_split'])
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=0,
        collate_fn=collate_fn
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("Creating model...")
    model = BoxRefinementModule(
        hidden_dim=config['model']['hidden_dim'],
        num_heads=config['model']['num_heads'],
        max_offset=config['model']['max_offset']
    ).to(device)
    
    # åˆ›å»ºHQ-SAMç‰¹å¾æå–å™¨
    print("Loading HQ-SAM feature extractor...")
    hqsam_extractor = create_hqsam_extractor(
        checkpoint_path=config['hqsam']['checkpoint'],
        model_type=config['hqsam']['model_type'],
        device=device,
        use_mock=True  # ä½¿ç”¨Mockç‰ˆæœ¬è¿›è¡Œæµ‹è¯•
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(
        model.parameters(),
        lr=float(config['training']['learning_rate']),
        weight_decay=float(config['training']['weight_decay'])
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if config['training']['lr_scheduler'] == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=config['training']['epochs']
        )
    else:
        scheduler = None
    
    # è®­ç»ƒå¾ªç¯
    print("Starting training...")
    best_iou = 0
    use_amp = config['training'].get('use_amp', False)
    
    # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
    training_start_time = time.time()
    
    for epoch in range(config['training']['epochs']):
        epoch_start_time = time.time()
        
        # è®­ç»ƒ
        train_loss, train_l1, train_iou = train_one_epoch(
            model, train_loader, optimizer, hqsam_extractor, device, epoch, config,
            feature_cache=feature_cache, use_amp=use_amp
        )
        
        epoch_time = time.time() - epoch_start_time
        
        # è¯„ä¼°
        if epoch % config['evaluation']['val_freq'] == 0:
            val_loss, val_l1, val_iou, val_bbox_iou = evaluate(
                model, val_loader, hqsam_extractor, device, config, 
                feature_cache=FeatureCache(config['output']['save_dir'], config['data']['val_split'])
            )
            
            logging.info(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val IoU={val_bbox_iou:.4f}, Time={epoch_time:.1f}s")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_bbox_iou > best_iou:
                best_iou = val_bbox_iou
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_iou': best_iou,
                    'config': config
                }, Path(config['output']['checkpoint_dir']) / 'best_model.pth')
        else:
            logging.info(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Time={epoch_time:.1f}s")
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None:
            scheduler.step()
        
        # å®šæœŸä¿å­˜
        if epoch % config['output']['save_freq'] == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'config': config
            }, Path(config['output']['checkpoint_dir']) / f'checkpoint_epoch_{epoch}.pth')
        
        # æ‰“å°ç¼“å­˜ç»Ÿè®¡
        if feature_cache:
            cache_stats = feature_cache.get_cache_stats()
            if cache_stats['total'] > 0:
                print(f"  Cache stats: {cache_stats['hits']}/{cache_stats['total']} hits ({cache_stats['hit_rate']:.1%})")
    
    total_training_time = time.time() - training_start_time
    print(f"Training completed! Best IoU: {best_iou:.4f}")
    print(f"Total training time: {total_training_time:.1f}s")
    
    # æ‰“å°æœ€ç»ˆç¼“å­˜ç»Ÿè®¡
    if feature_cache:
        final_cache_stats = feature_cache.get_cache_stats()
        print(f"Final cache stats: {final_cache_stats}")


if __name__ == '__main__':
    main()