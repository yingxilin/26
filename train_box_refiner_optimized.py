#!/usr/bin/env python3
"""
Box Refinement è®­ç»ƒè„šæœ¬ - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå·²ä¿®æ­£ï¼šæ›´å¥å£®çš„ç©ºæ•°æ®é›†æ£€æµ‹ä¸è¯Šæ–­ä¿¡æ¯ï¼‰
"""

import os
import sys
import time
import yaml
import argparse
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging
import platform

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm
import cv2

# æ·»åŠ modulesç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'modules'))

from modules.box_refinement import BoxRefinementModule, box_iou_loss
from modules.hqsam_feature_extractor import create_hqsam_extractor


class FungiDataset(Dataset):
    """FungiTastic æ•°æ®é›†åŠ è½½å™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, data_root: str, split: str = 'train', 
                 image_size: int = 300, data_subset: str = 'Mini',
                 augmentation: bool = True, debug: bool = False,
                 masks_file: Optional[str] = None):
        self.data_root = Path(data_root)
        self.split = split
        self.image_size = image_size
        self.data_subset = data_subset
        self.augmentation = augmentation
        self.debug = debug
        
        self.images_dir = self.data_root / f"{data_subset}" / split / f"{image_size}p"

        # å¦‚æä¾›parquetè·¯å¾„åˆ™ä½¿ç”¨
        if masks_file:
            self.masks_path = Path(masks_file)
            if not self.masks_path.exists():
                raise FileNotFoundError(f"Masks parquet file not found: {self.masks_path}")
            self.use_parquet_masks = True
            # ä½¿ç”¨ pyarrow.dataset è¿›è¡ŒæŒ‰éœ€è¯»å–
            try:
                import pyarrow.dataset as ds
                self._pa_ds = ds.dataset(str(self.masks_path), format='parquet')
                self._pa_schema_names = set(self._pa_ds.schema.names)
                self._pa_has_mask = 'mask' in self._pa_schema_names
                self._pa_has_rle = all(name in self._pa_schema_names for name in ['rle', 'width', 'height'])
                self._pa_file_name_field = 'file_name' if 'file_name' in self._pa_schema_names else None
            except Exception as e:
                raise RuntimeError(f"Failed to open parquet dataset: {e}")
            # ç¼“å­˜æœºåˆ¶
            self._mask_cache = {}
            self._mask_cache_limit = 1024
        else:
            self.masks_dir = self.data_root / f"{data_subset}" / split / "masks"
            # ä¸å†åœ¨è¿™é‡Œç›´æ¥æŠ›å‡ºï¼›ä½†ä¼šåœ¨åé¢æ ¹æ® images_dir æ£€æŸ¥ç»¼åˆæŠ¥é”™
            self.use_parquet_masks = False

        # è·å–å›¾åƒæ–‡ä»¶åˆ—è¡¨
        self.image_files = sorted(list(self.images_dir.glob("*.jpg")) + 
                                 list(self.images_dir.glob("*.JPG")) + 
                                 list(self.images_dir.glob("*.png")))
        
        if debug:
            self.image_files = self.image_files[:100]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾åƒï¼Œæä¾›è¯¦ç»†çš„è¯Šæ–­æç¤ºå¹¶æŠ›å‡ºé”™è¯¯ï¼ˆé¿å… DataLoader åç»­å´©æºƒï¼‰
        if len(self.image_files) == 0:
            expected_img_dir = str(self.images_dir)
            expected_masks_dir = str(self.masks_dir) if not getattr(self, 'use_parquet_masks', False) else str(self.masks_path)
            msg_lines = [
                f"No images found for split '{self.split}'.",
                f"Expected images directory: {expected_img_dir}",
                f"Expected masks (dir or parquet): {expected_masks_dir}",
                "Common causes:",
                " - config['data']['data_root'], data_subset, image_size or split is incorrect",
                " - images are stored in a different subfolder than '<data_subset>/<split>/<image_size>p'",
                " - file extensions differ (e.g. .jpeg) or filenames are not standard",
                "Please verify your dataset layout and config file."
            ]
            raise FileNotFoundError("\n".join(msg_lines))
        
        print(f"Found {len(self.image_files)} images in {self.split} split")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image_path = self.image_files[idx]
        image = cv2.imread(str(image_path))
        if image is None:
            raise ValueError(f"Failed to load image: {image_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        gt_bbox = None
        if self.use_parquet_masks:
            # ä¼˜å…ˆä»ç¼“å­˜è·å–
            image_key = image_path.name
            cached = self._mask_cache.get(image_key)
            if cached is not None:
                gt_bbox = cached
            else:
                try:
                    import pyarrow.dataset as ds
                except Exception as e:
                    raise RuntimeError(f"pyarrow is required for parquet reading: {e}")
                
                # ä»…è¯·æ±‚å®é™…å­˜åœ¨çš„åˆ—
                requested_cols = []
                if self._pa_file_name_field is not None:
                    requested_cols.append(self._pa_file_name_field)
                if self._pa_has_mask:
                    requested_cols.append('mask')
                if self._pa_has_rle:
                    requested_cols.extend(['width', 'height', 'rle'])
                
                if self._pa_file_name_field is not None:
                    filter_expr = (ds.field(self._pa_file_name_field) == image_key)
                else:
                    filter_expr = None
                
                table = self._pa_ds.to_table(columns=requested_cols, filter=filter_expr) if filter_expr is not None else self._pa_ds.to_table(columns=requested_cols)
                
                if table.num_rows == 0 and self._pa_file_name_field is not None:
                    filter_expr2 = (ds.field(self._pa_file_name_field) == image_path.stem)
                    table = self._pa_ds.to_table(columns=requested_cols, filter=filter_expr2)
                
                if table.num_rows == 0:
                    gt_bbox = None
                else:
                    cols = {name: table.column(name) for name in table.schema.names}
                    if self._pa_has_rle and all(k in cols for k in ['rle', 'width', 'height']):
                        rle_list = cols['rle'][0].as_py()
                        width_val = int(cols['width'][0].as_py())
                        height_val = int(cols['height'][0].as_py())
                        gt_bbox = self._compute_bbox_from_rle_counts(rle_list, width_val, height_val)
                    elif self._pa_has_mask and 'mask' in cols:
                        cell = cols['mask'][0].as_py()
                        mask_arr = np.asarray(cell)
                        if mask_arr.ndim == 3:
                            mask_arr = mask_arr[..., 0]
                        if mask_arr.ndim == 1:
                            mask_arr = mask_arr.reshape(image.shape[0], image.shape[1])
                        gt_bbox = self._compute_bbox_from_mask(mask_arr.astype(np.uint8))
                
                # ç¼“å­˜bbox
                if gt_bbox is None:
                    gt_bbox = self._compute_bbox_from_mask(np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8))
                if len(self._mask_cache) >= self._mask_cache_limit:
                    self._mask_cache.pop(next(iter(self._mask_cache)))
                self._mask_cache[image_key] = gt_bbox
        else:
            mask_path = self.masks_dir / f"{image_path.stem}.png"
            if not mask_path.exists():
                mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
                gt_bbox = self._compute_bbox_from_mask(mask)
            else:
                mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
            if mask is None:
                mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)
            if gt_bbox is None:
                gt_bbox = self._compute_bbox_from_mask(mask)
        
        # è°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œå¹¶æŒ‰æ¯”ä¾‹ç¼©æ”¾ bbox
        orig_h, orig_w = image.shape[:2]
        if image.shape[:2] != (self.image_size, self.image_size):
            sx = self.image_size / orig_w
            sy = self.image_size / orig_h
            image = cv2.resize(image, (self.image_size, self.image_size))
            if gt_bbox is not None:
                x1, y1, x2, y2 = gt_bbox
                gt_bbox = np.array([x1 * sx, y1 * sy, x2 * sx, y2 * sy], dtype=np.float32)
            mask = np.zeros((self.image_size, self.image_size), dtype=np.uint8)
        else:
            mask = np.zeros((orig_h, orig_w), dtype=np.uint8)
        
        # è‹¥ä»æœªå¾—åˆ°bboxï¼Œå…œåº•ä½¿ç”¨ç©ºmaskè§„åˆ™
        if gt_bbox is None:
            gt_bbox = self._compute_bbox_from_mask(mask)
        
        # ç”Ÿæˆnoisy bbox (æ¨¡æ‹ŸYOLOè¾“å‡º)
        noisy_bbox = self._generate_noisy_bbox(gt_bbox, image.shape[:2])
        
        # å½’ä¸€åŒ–è¾¹ç•Œæ¡†åæ ‡åˆ° [0, 1] èŒƒå›´
        h, w = image.shape[:2]
        gt_bbox_normalized = gt_bbox / np.array([w, h, w, h], dtype=np.float32)
        noisy_bbox_normalized = noisy_bbox / np.array([w, h, w, h], dtype=np.float32)
        
        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        gt_bbox_normalized = np.clip(gt_bbox_normalized, 0.0, 1.0)
        noisy_bbox_normalized = np.clip(noisy_bbox_normalized, 0.0, 1.0)
        
        # è½¬ä¸º float32 numpyï¼ˆDataLoader é»˜è®¤ collate ä¼šè½¬æ¢ä¸º tensorsï¼‰
        return {
            'image': image.astype(np.float32).transpose(2, 0, 1),  # CHW, float32ï¼Œä¾¿äºåç»­è½¬ torch.tensor
            'mask': mask.astype(np.uint8),
            'gt_bbox': gt_bbox_normalized.astype(np.float32),
            'noisy_bbox': noisy_bbox_normalized.astype(np.float32),
            'image_path': str(image_path)
        }
    
    def _compute_bbox_from_mask(self, mask):
        """ä»maskè®¡ç®—è¾¹ç•Œæ¡†"""
        if mask.sum() == 0:
            h, w = mask.shape
            center_x, center_y = w // 2, h // 2
            size = min(w, h) // 4
            return np.array([center_x - size, center_y - size, center_x + size, center_y + size], dtype=np.float32)
        
        coords = np.where(mask > 0)
        if len(coords[0]) == 0:
            h, w = mask.shape
            center_x, center_y = w // 2, h // 2
            size = min(w, h) // 4
            return np.array([center_x - size, center_y - size, center_x + size, center_y + size], dtype=np.float32)
        
        y_min, y_max = coords[0].min(), coords[0].max()
        x_min, x_max = coords[1].min(), coords[1].max()
        
        return np.array([x_min, y_min, x_max, y_max], dtype=np.float32)
    
    def _compute_bbox_from_rle_counts(self, rle_list, width, height):
        """ä»RLE countsè®¡ç®—è¾¹ç•Œæ¡†"""
        if not rle_list:
            return self._compute_bbox_from_mask(np.zeros((height, width), dtype=np.uint8))
        
        mask = np.zeros((height, width), dtype=np.uint8)
        pos = 0
        for i, count in enumerate(rle_list):
            if i % 2 == 0:
                end_pos = pos + count
                if end_pos <= height * width:
                    y = pos // width
                    x = pos % width
                    mask[y, x:x+count] = 1
            pos += count
        
        return self._compute_bbox_from_mask(mask)
    
    def _generate_noisy_bbox(self, gt_bbox, image_shape):
        """ç”Ÿæˆå¸¦å™ªå£°çš„è¾¹ç•Œæ¡†"""
        if gt_bbox is None:
            h, w = image_shape[:2]
            center_x, center_y = w // 2, h // 2
            size = min(w, h) // 4
            gt_bbox = np.array([center_x - size, center_y - size, center_x + size, center_y + size], dtype=np.float32)
        
        # æ·»åŠ éšæœºå™ªå£°
        noise_scale = 0.1
        h, w = image_shape[:2]
        max_noise = min(w, h) * noise_scale
        
        noise = np.random.uniform(-max_noise, max_noise, 4)
        noisy_bbox = gt_bbox + noise
        
        # ç¡®ä¿è¾¹ç•Œæ¡†åœ¨å›¾åƒèŒƒå›´å†…
        noisy_bbox[0] = max(0, min(noisy_bbox[0], w - 1))
        noisy_bbox[1] = max(0, min(noisy_bbox[1], h - 1))
        noisy_bbox[2] = max(noisy_bbox[0] + 1, min(noisy_bbox[2], w))
        noisy_bbox[3] = max(noisy_bbox[1] + 1, min(noisy_bbox[3], h))
        
        return noisy_bbox.astype(np.float32)


class FeatureCache:
    """ç‰¹å¾ç¼“å­˜ç®¡ç†å™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, cache_dir: str, split: str = 'train'):
        self.cache_dir = Path(cache_dir) / f"features/{split}"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.split = split
        self.cache_hits = 0
        self.cache_misses = 0
        self.total_requests = 0
    
    def get_cache_path(self, image_path: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        image_hash = hashlib.md5(image_path.encode()).hexdigest()
        return self.cache_dir / f"{image_hash}.npy"
    
    def load_features(self, image_path: str) -> Optional[torch.Tensor]:
        """ä»ç¼“å­˜åŠ è½½ç‰¹å¾"""
        self.total_requests += 1
        cache_path = self.get_cache_path(image_path)
        
        if cache_path.exists():
            try:
                features = np.load(cache_path)
                features_tensor = torch.from_numpy(features)
                self.cache_hits += 1
                return features_tensor
            except Exception as e:
                print(f"Warning: Failed to load cached features from {cache_path}: {e}")
                self.cache_misses += 1
                return None
        else:
            self.cache_misses += 1
            return None
    
    def save_features(self, image_path: str, features: torch.Tensor):
        """ä¿å­˜ç‰¹å¾åˆ°ç¼“å­˜"""
        cache_path = self.get_cache_path(image_path)
        try:
            features_cpu = features.cpu().numpy()
            np.save(cache_path, features_cpu)
        except Exception as e:
            print(f"Warning: Failed to save features to {cache_path}: {e}")
    
    def get_cache_stats(self) -> Dict[str, float]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.total_requests == 0:
            return {'hit_rate': 0.0, 'hits': 0, 'misses': 0}
        
        hit_rate = self.cache_hits / self.total_requests
        return {
            'hit_rate': hit_rate,
            'hits': self.cache_hits,
            'misses': self.cache_misses
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.cache_dir.exists():
            import shutil
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_hits = 0
        self.cache_misses = 0
        self.total_requests = 0


def detect_feature_cache(cache_dir: str) -> bool:
    """æ£€æµ‹æ˜¯å¦å­˜åœ¨ç‰¹å¾ç¼“å­˜"""
    cache_path = Path(cache_dir) / "features"
    return cache_path.exists() and any(cache_path.iterdir())


def extract_features_with_cache(hqsam_extractor, images_np_list, image_paths, feature_cache, device='cuda'):
    """ä½¿ç”¨ç¼“å­˜æå–ç‰¹å¾ - ä¼˜åŒ–ç‰ˆæœ¬"""
    features_list = []
    uncached_indices = []
    uncached_images = []
    
    # é¦–å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½æ‰€æœ‰ç‰¹å¾
    for i, (image_np, image_path) in enumerate(zip(images_np_list, image_paths)):
        if feature_cache is not None:
            cached_features = feature_cache.load_features(image_path)
            if cached_features is not None:
                cached_features = cached_features.to(device)
                features_list.append(cached_features)
                continue
        
        # è®°å½•éœ€è¦æå–ç‰¹å¾çš„å›¾åƒ
        uncached_indices.append(i)
        uncached_images.append(image_np)
        features_list.append(None)
    
    # æ‰¹é‡æå–æœªç¼“å­˜çš„ç‰¹å¾
    if uncached_images:
        batch_features = hqsam_extractor.extract_features_batch(uncached_images)
        
        # å°†æå–çš„ç‰¹å¾æ”¾å›æ­£ç¡®ä½ç½®
        for idx, features in zip(uncached_indices, batch_features):
            features_list[idx] = features
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if feature_cache is not None:
                feature_cache.save_features(image_paths[idx], features)
    
    return features_list


def compute_loss(pred_bboxes, gt_bboxes, l1_weight=1.0, iou_weight=0.5):
    """è®¡ç®—æŸå¤±å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
    # ç¡®ä¿è¾“å…¥å¼ é‡åœ¨ç›¸åŒè®¾å¤‡ä¸Š
    if pred_bboxes.device != gt_bboxes.device:
        gt_bboxes = gt_bboxes.to(pred_bboxes.device)
    
    # ç¡®ä¿è¾“å…¥å¼ é‡å½¢çŠ¶ä¸€è‡´
    if pred_bboxes.shape != gt_bboxes.shape:
        min_batch = min(pred_bboxes.shape[0], gt_bboxes.shape[0])
        pred_bboxes = pred_bboxes[:min_batch]
        gt_bboxes = gt_bboxes[:min_batch]
    
    # æ£€æŸ¥è¾“å…¥æœ‰æ•ˆæ€§
    if pred_bboxes.numel() == 0 or gt_bboxes.numel() == 0:
        return torch.tensor(0.0, device=pred_bboxes.device), torch.tensor(0.0, device=pred_bboxes.device), torch.tensor(0.0, device=pred_bboxes.device)
    
    # L1æŸå¤±
    l1_loss = F.l1_loss(pred_bboxes, gt_bboxes)
    
    # IoUæŸå¤±
    try:
        iou_loss = box_iou_loss(pred_bboxes, gt_bboxes)
        if torch.isnan(iou_loss) or torch.isinf(iou_loss):
            iou_loss = torch.tensor(0.0, device=pred_bboxes.device)
    except Exception as e:
        print(f"Warning: IoU loss computation failed: {e}")
        iou_loss = torch.tensor(0.0, device=pred_bboxes.device)
    
    # æ€»æŸå¤±
    total_loss = l1_weight * l1_loss + iou_weight * iou_loss
    
    return total_loss, l1_loss, iou_loss


def train_one_epoch(model, dataloader, optimizer, hqsam_extractor, device, epoch, config, 
                   feature_cache=None, use_amp=False):
    """è®­ç»ƒä¸€ä¸ªepoch - ä¼˜åŒ–ç‰ˆæœ¬"""
    model.train()
    
    total_loss = 0.0
    total_l1_loss = 0.0
    total_iou_loss = 0.0
    num_batches = len(dataloader)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler() if use_amp else None
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
    
    for batch_idx, batch in enumerate(pbar):
        # è·å–æ•°æ®
        # batch å­—æ®µå¯èƒ½æ˜¯ numpyï¼Œéœ€è¦è½¬æ¢ä¸º torch.tensor
        images = torch.tensor(batch['image']).to(device)  # (B, C, H, W)
        gt_bboxes = torch.tensor(batch['gt_bbox']).to(device)
        noisy_bboxes = torch.tensor(batch['noisy_bbox']).to(device)
        image_paths = batch['image_path']
        
        # ç¡®ä¿image_pathsæ˜¯åˆ—è¡¨
        if isinstance(image_paths, str):
            image_paths = [image_paths]
        
        # å°† images è½¬å› HWC numpy åˆ—è¡¨ç”¨äºç‰¹å¾æå–ï¼ˆextractor æ¥å— HWC numpyï¼‰
        images_np_list = [img.cpu().numpy().transpose(1, 2, 0) for img in images]
        features_list = extract_features_with_cache(
            hqsam_extractor, images_np_list, image_paths, feature_cache, device
        )
        image_features = torch.cat(features_list, dim=0)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        
        if use_amp and scaler is not None:
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast():
                # è¿­ä»£ç²¾ç‚¼
                refined_bboxes, history = model.iterative_refine(
                    image_features, noisy_bboxes, (config['data']['image_size'], config['data']['image_size']),
                    max_iter=config['refinement']['max_iter'],
                    stop_threshold=config['refinement']['stop_threshold']
                )
                
                # è®¡ç®—æŸå¤±
                loss, l1_loss, iou_loss = compute_loss(
                    refined_bboxes, gt_bboxes,
                    l1_weight=config['loss']['l1_weight'],
                    iou_weight=config['loss']['iou_weight']
                )
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            # æ¢¯åº¦è£å‰ª
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            # æ™®é€šç²¾åº¦å‰å‘ä¼ æ’­
            # è¿­ä»£ç²¾ç‚¼
            refined_bboxes, history = model.iterative_refine(
                image_features, noisy_bboxes, (config['data']['image_size'], config['data']['image_size']),
                max_iter=config['refinement']['max_iter'],
                stop_threshold=config['refinement']['stop_threshold']
            )
            
            # è®¡ç®—æŸå¤±
            loss, l1_loss, iou_loss = compute_loss(
                refined_bboxes, gt_bboxes,
                l1_weight=config['loss']['l1_weight'],
                iou_weight=config['loss']['iou_weight']
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        
        # æ›´æ–°ç»Ÿè®¡
        total_loss += loss.item()
        total_l1_loss += l1_loss.item()
        total_iou_loss += iou_loss.item()
        
        # æ›´æ–°è¿›åº¦æ¡
        cache_stats = feature_cache.get_cache_stats() if feature_cache else {'hit_rate': 0.0}
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'L1': f'{l1_loss.item():.4f}',
            'IoU': f'{iou_loss.item():.4f}',
            'Cache': f'{cache_stats["hit_rate"]:.1%}'
        })
    
    return total_loss / num_batches, total_l1_loss / num_batches, total_iou_loss / num_batches


def evaluate(model, dataloader, hqsam_extractor, device, config, feature_cache=None):
    """è¯„ä¼°æ¨¡å‹ - ä¼˜åŒ–ç‰ˆæœ¬"""
    model.eval()
    
    total_loss = 0.0
    total_l1_loss = 0.0
    total_iou_loss = 0.0
    num_batches = len(dataloader)
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            images = torch.tensor(batch['image']).to(device)
            gt_bboxes = torch.tensor(batch['gt_bbox']).to(device)
            noisy_bboxes = torch.tensor(batch['noisy_bbox']).to(device)
            image_paths = batch['image_path']
            
            if isinstance(image_paths, str):
                image_paths = [image_paths]
            
            images_np_list = [img.cpu().numpy().transpose(1, 2, 0) for img in images]
            features_list = extract_features_with_cache(
                hqsam_extractor, images_np_list, image_paths, feature_cache, device
            )
            image_features = torch.cat(features_list, dim=0)
            
            refined_bboxes, history = model.iterative_refine(
                image_features, noisy_bboxes, (config['data']['image_size'], config['data']['image_size']),
                max_iter=config['refinement']['max_iter'],
                stop_threshold=config['refinement']['stop_threshold']
            )
            
            loss, l1_loss, iou_loss = compute_loss(
                refined_bboxes, gt_bboxes,
                l1_weight=config['loss']['l1_weight'],
                iou_weight=config['loss']['iou_weight']
            )
            
            total_loss += loss.item()
            total_l1_loss += l1_loss.item()
            total_iou_loss += iou_loss.item()
    
    return total_loss / num_batches, total_l1_loss / num_batches, total_iou_loss / num_batches


def main():
    """ä¸»å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
    parser = argparse.ArgumentParser(description='Box Refinement Training - Optimized Version')
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡é€‰æ‹©')
    parser.add_argument('--fast', action='store_true', help='å¿«é€Ÿæ¨¡å¼')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--clear-cache', action='store_true', help='æ¸…ç©ºç‰¹å¾ç¼“å­˜')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å¿«é€Ÿæ¨¡å¼è®¾ç½®
    if args.fast:
        print("ğŸš€ Fast mode enabled - applying all optimizations")
        config['data']['sample_ratio'] = 0.1
        config['training']['use_amp'] = True
        config['training']['batch_size'] = 32
        print(f"  - Data sampling: {config['data']['sample_ratio']}")
        print(f"  - Mixed precision: {config['training']['use_amp']}")
        print(f"  - Batch size: {config['training']['batch_size']}")
    
    # è®¾å¤‡é€‰æ‹©
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print(f"Using device: {device}")
    
    # æ£€æµ‹ç‰¹å¾ç¼“å­˜
    cache_detected = detect_feature_cache(config['output']['checkpoint_dir'])
    print(f"Feature cache detected: {cache_detected}")
    
    # åˆ›å»ºç‰¹å¾ç¼“å­˜
    feature_cache = FeatureCache(config['output']['checkpoint_dir']) if config['training'].get('feature_cache', False) else None
    
    # æ¸…ç©ºç¼“å­˜
    if args.clear_cache and feature_cache is not None:
        print("Clearing feature cache...")
        feature_cache.clear_cache()
        print("Feature cache cleared.")
    
    # åŠ è½½æ•°æ®é›†
    print("Loading datasets...")
    train_dataset = FungiDataset(
        data_root=config['data']['data_root'],
        split=config['data']['train_split'],
        image_size=config['data']['image_size'],
        data_subset=config['data']['data_subset'],
        augmentation=config['data']['augmentation']['enabled'],
        debug=args.debug,
        masks_file=config['data'].get('masks_file')
    )
    
    val_dataset = FungiDataset(
        data_root=config['data']['data_root'],
        split=config['data']['val_split'],
        image_size=config['data']['image_size'],
        data_subset=config['data']['data_subset'],
        augmentation=False,
        debug=args.debug,
        masks_file=config['data'].get('masks_file')
    )
    
    # æ•°æ®æŠ½æ ·
    if config['data'].get('sample_ratio') is not None:
        sample_ratio = config['data']['sample_ratio']
        if sample_ratio < 1.0:
            # å¯¹è®­ç»ƒé›†è¿›è¡ŒæŠ½æ ·
            original_train_len = len(train_dataset)
            train_size = int(original_train_len * sample_ratio)
            if train_size <= 0:
                raise RuntimeError(f"Sample ratio {sample_ratio} produced zero train samples (original={original_train_len}). "
                                   "Please check `data.sample_ratio` and dataset content.")
            train_indices = torch.randperm(original_train_len)[:train_size]
            train_dataset = torch.utils.data.Subset(train_dataset, train_indices)
            print(f"Sampled {len(train_dataset)} images from {original_train_len} total images (ratio: {sample_ratio})")
            
            # å¯¹éªŒè¯é›†è¿›è¡ŒæŠ½æ ·
            original_val_len = len(val_dataset)
            val_size = int(original_val_len * sample_ratio)
            if val_size <= 0:
                raise RuntimeError(f"Sample ratio {sample_ratio} produced zero val samples (original={original_val_len}). "
                                   "Please check `data.sample_ratio` and dataset content.")
            val_indices = torch.randperm(original_val_len)[:val_size]
            val_dataset = torch.utils.data.Subset(val_dataset, val_indices)
            print(f"Sampled {len(val_dataset)} images from {original_val_len} total images (ratio: {sample_ratio})")
    
    # åœ¨åˆ›å»º DataLoader ä¹‹å‰ï¼Œå†æ¬¡æ£€æŸ¥æ•°æ®é›†é•¿åº¦ï¼ˆé¿å… RandomSampler num_samples=0ï¼‰
    if len(train_dataset) == 0:
        raise RuntimeError("Train dataset is empty after sampling. Please check dataset paths and config. "
                           f"Train data root: {config['data']['data_root']}, subset: {config['data']['data_subset']}, "
                           f"train_split: {config['data']['train_split']}, image_size: {config['data']['image_size']}")
    if len(val_dataset) == 0:
        raise RuntimeError("Validation dataset is empty after sampling. Please check dataset paths and config. "
                           f"Val data root: {config['data']['data_root']}, subset: {config['data']['data_subset']}, "
                           f"val_split: {config['data']['val_split']}, image_size: {config['data']['image_size']}")
    
    # å…¼å®¹ Windows/persistent_workers è®¾ç½®ï¼ˆWindows ä¸‹ persistent_workers True æœ‰æ—¶ä¼šå‡ºé—®é¢˜ï¼‰
    num_workers = int(config['data'].get('num_workers', 0))
    persistent_workers_flag = bool(config['data'].get('persistent_workers', False))
    if platform.system() == "Windows" and num_workers == 0:
        persistent_workers_flag = False
    # å¦‚æœ num_workers == 0ï¼Œä¹Ÿå¿…é¡»ç¦ç”¨ persistent_workers
    if num_workers == 0:
        persistent_workers_flag = False
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=persistent_workers_flag
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=persistent_workers_flag
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("Creating model...")
    model = BoxRefinementModule(
        hidden_dim=config['model']['hidden_dim'],
        num_heads=config['model']['num_heads'],
        max_offset=config['model']['max_offset']
    ).to(device)
    
    # åˆ›å»ºHQ-SAMç‰¹å¾æå–å™¨
    print("Loading HQ-SAM feature extractor...")
    hqsam_extractor = create_hqsam_extractor(
        checkpoint_path=config['hqsam']['checkpoint'],
        model_type=config['hqsam']['model_type'],
        device=device,
        use_mock=True  # ä½¿ç”¨Mockç‰ˆæœ¬è¿›è¡Œæµ‹è¯•
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    print("Creating optimizer...")
    learning_rate = float(config['training']['learning_rate'])
    if args.fast:
        learning_rate *= 2  # å¿«é€Ÿæ¨¡å¼ä¸‹ç¨å¾®æé«˜å­¦ä¹ ç‡
    
    optimizer = optim.Adam(
        model.parameters(),
        lr=learning_rate,
        weight_decay=float(config['training']['weight_decay'])
    )
    
    print(f"Learning rate: {learning_rate}")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if config['training'].get('lr_scheduler') == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=config['training']['epochs']
        )
    else:
        scheduler = None
    
    # è®­ç»ƒå¾ªç¯
    print("Starting training...")
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    for epoch in range(config['training']['epochs']):
        # è®­ç»ƒ
        train_loss, train_l1, train_iou = train_one_epoch(
            model, train_loader, optimizer, hqsam_extractor, device, epoch, config,
            feature_cache=feature_cache, use_amp=config['training'].get('use_amp', False)
        )
        
        # éªŒè¯
        val_loss, val_l1, val_iou = evaluate(
            model, val_loader, hqsam_extractor, device, config, feature_cache=feature_cache
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        if scheduler is not None:
            scheduler.step()
        
        # è®°å½•æŸå¤±
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        print(f"  Train L1: {train_l1:.4f}, IoU: {train_iou:.4f}")
        print(f"  Val L1: {val_l1:.4f}, IoU: {val_iou:.4f}")
        
        # ç¼“å­˜ç»Ÿè®¡
        if feature_cache is not None:
            cache_stats = feature_cache.get_cache_stats()
            print(f"  Cache hit rate: {cache_stats['hit_rate']:.1%}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
            }, os.path.join(config['output']['checkpoint_dir'], 'best_model.pth'))
            print(f"  New best model saved! Val Loss: {val_loss:.4f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save({
        'epoch': config['training']['epochs'] - 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
    }, os.path.join(config['output']['checkpoint_dir'], 'final_model.pth'))
    
    print("Training completed!")
    print(f"Best validation loss: {best_val_loss:.4f}")
    
    # æ‰“å°ç¼“å­˜ç»Ÿè®¡
    if feature_cache is not None:
        cache_stats = feature_cache.get_cache_stats()
        print(f"Final cache hit rate: {cache_stats['hit_rate']:.1%}")
        print(f"Total cache hits: {cache_stats['hits']}")
        print(f"Total cache misses: {cache_stats['misses']}")


if __name__ == "__main__":
    main()
